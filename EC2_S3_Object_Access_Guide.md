# EC2ã‹ã‚‰S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—ã‚¬ã‚¤ãƒ‰ ğŸ“¦

## ğŸ“‹ æ¦‚è¦

EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰S3ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å®‰å…¨ã‹ã¤åŠ¹ç‡çš„ã«å–å¾—ã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚IAMãƒ­ãƒ¼ãƒ«è¨­å®šã‹ã‚‰å®Ÿè£…ä¾‹ã¾ã§ã€å®Ÿç”¨çš„ãªæ‰‹é †ã‚’æä¾›ã—ã¾ã™ã€‚

---

## ğŸ¯ ç›®æ¬¡

1. [IAMãƒ­ãƒ¼ãƒ«è¨­å®š](#-iamãƒ­ãƒ¼ãƒ«è¨­å®š)
2. [ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š](#-ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š)
3. [SDKå®Ÿè£…æ–¹æ³•](#-sdkå®Ÿè£…æ–¹æ³•)
4. [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](#-ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹)
5. [ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°](#-ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°)
6. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)
7. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#-ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

---

## ğŸ”‘ IAMãƒ­ãƒ¼ãƒ«è¨­å®š

### åŸºæœ¬çš„ãªS3èª­ã¿å–ã‚Šæ¨©é™

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "S3GetObjectAccess",
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:GetObjectVersion",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::your-bucket-name/*",
                "arn:aws:s3:::your-bucket-name"
            ]
        }
    ]
}
```

### è©³ç´°æ¨©é™è¨­å®š

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "S3BasicAccess",
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:GetObjectVersion",
                "s3:GetObjectMetadata",
                "s3:GetObjectAttributes",
                "s3:ListBucket",
                "s3:ListBucketVersions"
            ],
            "Resource": [
                "arn:aws:s3:::your-bucket-name/*",
                "arn:aws:s3:::your-bucket-name"
            ]
        },
        {
            "Sid": "S3ConditionalAccess",
            "Effect": "Allow",
            "Action": [
                "s3:GetObject"
            ],
            "Resource": "arn:aws:s3:::your-bucket-name/secure/*",
            "Condition": {
                "StringEquals": {
                    "s3:ExistingObjectTag/Environment": "Production"
                }
            }
        }
    ]
}
```

### IAMãƒ­ãƒ¼ãƒ«ä¿¡é ¼é–¢ä¿‚

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "ec2.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```

---

## ğŸ”— ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š

### CloudFormationãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä¾‹

```yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: 'EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”¨S3ã‚¢ã‚¯ã‚»ã‚¹è¨­å®š'

Resources:
  EC2S3AccessRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: EC2-S3-Access-Role
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
      Policies:
        - PolicyName: S3AccessPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:GetObjectVersion
                  - s3:ListBucket
                Resource:
                  - !Sub 'arn:aws:s3:::${S3BucketName}/*'
                  - !Sub 'arn:aws:s3:::${S3BucketName}'

  EC2InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      InstanceProfileName: EC2-S3-Instance-Profile
      Roles:
        - !Ref EC2S3AccessRole

  EC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: t3.micro
      ImageId: ami-0123456789abcdef0
      IamInstanceProfile: !Ref EC2InstanceProfile
      SecurityGroupIds:
        - !Ref InstanceSecurityGroup
      Tags:
        - Key: Name
          Value: S3-Access-Instance

Parameters:
  S3BucketName:
    Type: String
    Description: S3ãƒã‚±ãƒƒãƒˆå
    Default: your-s3-bucket-name
```

---

## ğŸ SDKå®Ÿè£…æ–¹æ³•

### Python (Boto3) å®Ÿè£…

#### åŸºæœ¬çš„ãªå®Ÿè£…

```python
import boto3
import logging
from botocore.exceptions import ClientError, NoCredentialsError
from typing import Dict, Any, Optional

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class S3ObjectRetriever:
    def __init__(self, region_name: str = 'ap-northeast-1'):
        """
        S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        
        Args:
            region_name: AWS ãƒªãƒ¼ã‚¸ãƒ§ãƒ³å
        """
        try:
            self.s3_client = boto3.client('s3', region_name=region_name)
            self.s3_resource = boto3.resource('s3', region_name=region_name)
            logger.info(f"S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†: {region_name}")
        except NoCredentialsError:
            logger.error("AWSèªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            raise
        except Exception as e:
            logger.error(f"S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

    def get_object_to_memory(self, bucket_name: str, object_key: str) -> Dict[str, Any]:
        """
        S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¿
        
        Args:
            bucket_name: S3ãƒã‚±ãƒƒãƒˆå
            object_key: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼
            
        Returns:
            ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        try:
            logger.info(f"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—é–‹å§‹: s3://{bucket_name}/{object_key}")
            
            response = self.s3_client.get_object(
                Bucket=bucket_name,
                Key=object_key
            )
            
            content = response['Body'].read()
            
            result = {
                'content': content,
                'metadata': {
                    'content_type': response.get('ContentType', ''),
                    'content_length': response.get('ContentLength', 0),
                    'last_modified': response.get('LastModified', ''),
                    'etag': response.get('ETag', ''),
                    'version_id': response.get('VersionId', '')
                }
            }
            
            logger.info(f"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—å®Œäº†: {len(content)} bytes")
            return result
            
        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code == 'NoSuchKey':
                logger.error(f"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“: {object_key}")
            elif error_code == 'NoSuchBucket':
                logger.error(f"ãƒã‚±ãƒƒãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“: {bucket_name}")
            elif error_code == 'AccessDenied':
                logger.error(f"ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: s3://{bucket_name}/{object_key}")
            else:
                logger.error(f"S3ã‚¨ãƒ©ãƒ¼: {error_code} - {e.response['Error']['Message']}")
            raise
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

    def download_file(self, bucket_name: str, object_key: str, local_file_path: str) -> bool:
        """
        S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        Args:
            bucket_name: S3ãƒã‚±ãƒƒãƒˆå
            object_key: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼
            local_file_path: ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ãƒ‘ã‚¹
            
        Returns:
            ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸå¯å¦
        """
        try:
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: s3://{bucket_name}/{object_key} -> {local_file_path}")
            
            self.s3_client.download_file(
                Bucket=bucket_name,
                Key=object_key,
                Filename=local_file_path
            )
            
            logger.info("ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            return True
            
        except ClientError as e:
            error_code = e.response['Error']['Code']
            logger.error(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {error_code}")
            return False
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

    def get_object_with_conditions(self, 
                                 bucket_name: str, 
                                 object_key: str,
                                 if_modified_since: Optional[str] = None,
                                 if_match: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """
        æ¡ä»¶ä»˜ãã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—
        
        Args:
            bucket_name: S3ãƒã‚±ãƒƒãƒˆå
            object_key: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼
            if_modified_since: æŒ‡å®šæ—¥æ™‚ä»¥é™ã«å¤‰æ›´ã•ã‚ŒãŸå ´åˆã®ã¿å–å¾—
            if_match: æŒ‡å®šETagã¨ä¸€è‡´ã™ã‚‹å ´åˆã®ã¿å–å¾—
            
        Returns:
            æ¡ä»¶ã«åˆè‡´ã—ãŸå ´åˆã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿
        """
        try:
            params = {
                'Bucket': bucket_name,
                'Key': object_key
            }
            
            if if_modified_since:
                params['IfModifiedSince'] = if_modified_since
            if if_match:
                params['IfMatch'] = if_match
                
            logger.info(f"æ¡ä»¶ä»˜ãå–å¾—é–‹å§‹: s3://{bucket_name}/{object_key}")
            
            response = self.s3_client.get_object(**params)
            
            return {
                'content': response['Body'].read(),
                'metadata': {
                    'content_type': response.get('ContentType', ''),
                    'content_length': response.get('ContentLength', 0),
                    'last_modified': response.get('LastModified', ''),
                    'etag': response.get('ETag', '')
                }
            }
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NotModified':
                logger.info("ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯å¤‰æ›´ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return None
            else:
                logger.error(f"æ¡ä»¶ä»˜ãå–å¾—ã‚¨ãƒ©ãƒ¼: {e.response['Error']['Code']}")
                raise
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

    def list_bucket_objects(self, bucket_name: str, prefix: str = '') -> Dict[str, Any]:
        """
        ãƒã‚±ãƒƒãƒˆå†…ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—
        
        Args:
            bucket_name: S3ãƒã‚±ãƒƒãƒˆå
            prefix: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            
        Returns:
            ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§
        """
        try:
            logger.info(f"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—: s3://{bucket_name}/{prefix}")
            
            paginator = self.s3_client.get_paginator('list_objects_v2')
            page_iterator = paginator.paginate(
                Bucket=bucket_name,
                Prefix=prefix
            )
            
            objects = []
            total_size = 0
            
            for page in page_iterator:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        objects.append({
                            'key': obj['Key'],
                            'size': obj['Size'],
                            'last_modified': obj['LastModified'],
                            'etag': obj['ETag']
                        })
                        total_size += obj['Size']
            
            result = {
                'objects': objects,
                'count': len(objects),
                'total_size': total_size
            }
            
            logger.info(f"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—å®Œäº†: {len(objects)}å€‹, {total_size} bytes")
            return result
            
        except ClientError as e:
            logger.error(f"ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e.response['Error']['Code']}")
            raise
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise


# ä½¿ç”¨ä¾‹
def main():
    """
    S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—ã®ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
    """
    try:
        # S3å–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        s3_retriever = S3ObjectRetriever()
        
        bucket_name = "your-bucket-name"
        object_key = "path/to/your/file.txt"
        
        # 1. ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¿
        result = s3_retriever.get_object_to_memory(bucket_name, object_key)
        print(f"å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(result['content'])} bytes")
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {result['metadata']}")
        
        # 2. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        local_path = "/tmp/downloaded_file.txt"
        success = s3_retriever.download_file(bucket_name, object_key, local_path)
        if success:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {local_path}")
        
        # 3. ãƒã‚±ãƒƒãƒˆå†…ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—
        objects_info = s3_retriever.list_bucket_objects(bucket_name, "documents/")
        print(f"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ•°: {objects_info['count']}")
        
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")


if __name__ == "__main__":
    main()
```

#### å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œå®Ÿè£…

```python
import boto3
from botocore.exceptions import ClientError
import threading
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

class LargeFileRetriever:
    def __init__(self, region_name: str = 'ap-northeast-1'):
        self.s3_client = boto3.client('s3', region_name=region_name)
        self.logger = logging.getLogger(__name__)
        
    def download_large_file_multipart(self, 
                                    bucket_name: str, 
                                    object_key: str, 
                                    local_file_path: str,
                                    chunk_size: int = 1024*1024*10,  # 10MB
                                    max_threads: int = 4) -> bool:
        """
        å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        Args:
            bucket_name: S3ãƒã‚±ãƒƒãƒˆå
            object_key: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼
            local_file_path: ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ãƒ‘ã‚¹
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰
            max_threads: æœ€å¤§ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
            
        Returns:
            ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸå¯å¦
        """
        try:
            # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚µã‚¤ã‚ºå–å¾—
            head_response = self.s3_client.head_object(Bucket=bucket_name, Key=object_key)
            file_size = head_response['ContentLength']
            
            self.logger.info(f"å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {file_size} bytes")
            
            if file_size <= chunk_size:
                # å°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯é€šå¸¸ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                self.s3_client.download_file(bucket_name, object_key, local_file_path)
                return True
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = []
            for start in range(0, file_size, chunk_size):
                end = min(start + chunk_size - 1, file_size - 1)
                chunks.append((start, end))
            
            # ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            with ThreadPoolExecutor(max_workers=max_threads) as executor:
                futures = []
                
                for i, (start, end) in enumerate(chunks):
                    future = executor.submit(
                        self._download_chunk,
                        bucket_name, object_key, start, end, i
                    )
                    futures.append(future)
                
                # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿åé›†
                chunk_data = {}
                for future in as_completed(futures):
                    chunk_index, data = future.result()
                    chunk_data[chunk_index] = data
            
            # ãƒ•ã‚¡ã‚¤ãƒ«çµåˆ
            with open(local_file_path, 'wb') as f:
                for i in range(len(chunks)):
                    f.write(chunk_data[i])
            
            self.logger.info("å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            return True
            
        except Exception as e:
            self.logger.error(f"å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _download_chunk(self, bucket_name: str, object_key: str, start: int, end: int, chunk_index: int):
        """
        ãƒãƒ£ãƒ³ã‚¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        """
        try:
            range_header = f"bytes={start}-{end}"
            response = self.s3_client.get_object(
                Bucket=bucket_name,
                Key=object_key,
                Range=range_header
            )
            
            return chunk_index, response['Body'].read()
            
        except Exception as e:
            self.logger.error(f"ãƒãƒ£ãƒ³ã‚¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ ({chunk_index}): {str(e)}")
            raise
```

### Node.js (AWS SDK v3) å®Ÿè£…

```javascript
import { 
    S3Client, 
    GetObjectCommand, 
    HeadObjectCommand,
    ListObjectsV2Command 
} from "@aws-sdk/client-s3";
import { fromInstanceMetadata } from "@aws-sdk/credential-providers";
import fs from 'fs';
import { pipeline } from 'stream/promises';

class S3ObjectRetriever {
    constructor(region = 'ap-northeast-1') {
        this.s3Client = new S3Client({
            region: region,
            credentials: fromInstanceMetadata({
                timeout: 5000,
                maxRetries: 3
            })
        });
        this.logger = console;
    }

    async getObjectToMemory(bucketName, objectKey) {
        try {
            this.logger.info(`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—é–‹å§‹: s3://${bucketName}/${objectKey}`);
            
            const command = new GetObjectCommand({
                Bucket: bucketName,
                Key: objectKey
            });
            
            const response = await this.s3Client.send(command);
            
            // Streamã‚’Bufferã«å¤‰æ›
            const chunks = [];
            for await (const chunk of response.Body) {
                chunks.push(chunk);
            }
            const buffer = Buffer.concat(chunks);
            
            const result = {
                content: buffer,
                metadata: {
                    contentType: response.ContentType || '',
                    contentLength: response.ContentLength || 0,
                    lastModified: response.LastModified || '',
                    etag: response.ETag || ''
                }
            };
            
            this.logger.info(`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—å®Œäº†: ${buffer.length} bytes`);
            return result;
            
        } catch (error) {
            if (error.name === 'NoSuchKey') {
                this.logger.error(`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“: ${objectKey}`);
            } else if (error.name === 'NoSuchBucket') {
                this.logger.error(`ãƒã‚±ãƒƒãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“: ${bucketName}`);
            } else if (error.name === 'AccessDenied') {
                this.logger.error(`ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: s3://${bucketName}/${objectKey}`);
            } else {
                this.logger.error(`S3ã‚¨ãƒ©ãƒ¼: ${error.message}`);
            }
            throw error;
        }
    }

    async downloadFile(bucketName, objectKey, localFilePath) {
        try {
            this.logger.info(`ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: s3://${bucketName}/${objectKey} -> ${localFilePath}`);
            
            const command = new GetObjectCommand({
                Bucket: bucketName,
                Key: objectKey
            });
            
            const response = await this.s3Client.send(command);
            
            const writeStream = fs.createWriteStream(localFilePath);
            await pipeline(response.Body, writeStream);
            
            this.logger.info('ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†');
            return true;
            
        } catch (error) {
            this.logger.error(`ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: ${error.message}`);
            return false;
        }
    }

    async listBucketObjects(bucketName, prefix = '') {
        try {
            this.logger.info(`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—: s3://${bucketName}/${prefix}`);
            
            const objects = [];
            let totalSize = 0;
            let continuationToken = undefined;
            
            do {
                const command = new ListObjectsV2Command({
                    Bucket: bucketName,
                    Prefix: prefix,
                    ContinuationToken: continuationToken
                });
                
                const response = await this.s3Client.send(command);
                
                if (response.Contents) {
                    for (const obj of response.Contents) {
                        objects.push({
                            key: obj.Key,
                            size: obj.Size,
                            lastModified: obj.LastModified,
                            etag: obj.ETag
                        });
                        totalSize += obj.Size;
                    }
                }
                
                continuationToken = response.NextContinuationToken;
                
            } while (continuationToken);
            
            const result = {
                objects: objects,
                count: objects.length,
                totalSize: totalSize
            };
            
            this.logger.info(`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—å®Œäº†: ${objects.length}å€‹, ${totalSize} bytes`);
            return result;
            
        } catch (error) {
            this.logger.error(`ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: ${error.message}`);
            throw error;
        }
    }
}

// ä½¿ç”¨ä¾‹
async function main() {
    try {
        const s3Retriever = new S3ObjectRetriever();
        
        const bucketName = "your-bucket-name";
        const objectKey = "path/to/your/file.txt";
        
        // ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¿
        const result = await s3Retriever.getObjectToMemory(bucketName, objectKey);
        console.log(`å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: ${result.content.length} bytes`);
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        const localPath = "/tmp/downloaded_file.txt";
        const success = await s3Retriever.downloadFile(bucketName, objectKey, localPath);
        if (success) {
            console.log(`ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: ${localPath}`);
        }
        
        // ãƒã‚±ãƒƒãƒˆå†…ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—
        const objectsInfo = await s3Retriever.listBucketObjects(bucketName, "documents/");
        console.log(`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ•°: ${objectsInfo.count}`);
        
    } catch (error) {
        console.error(`ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: ${error.message}`);
    }
}

main();
```

---

## ğŸ›¡ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### æœ€å°æ¨©é™ã®åŸå‰‡

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "RestrictedS3Access",
            "Effect": "Allow",
            "Action": [
                "s3:GetObject"
            ],
            "Resource": "arn:aws:s3:::your-bucket-name/allowed-prefix/*",
            "Condition": {
                "StringEquals": {
                    "s3:ExistingObjectTag/AccessLevel": "EC2Allowed"
                },
                "DateGreaterThan": {
                    "aws:CurrentTime": "2024-01-01T00:00:00Z"
                },
                "IpAddress": {
                    "aws:SourceIp": ["10.0.0.0/8", "172.16.0.0/12"]
                }
            }
        }
    ]
}
```

### VPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçµŒç”±ã‚¢ã‚¯ã‚»ã‚¹

```yaml
# VPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
S3VPCEndpoint:
  Type: AWS::EC2::VPCEndpoint
  Properties:
    VpcId: !Ref VPC
    ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'
    VpcEndpointType: Gateway
    RouteTableIds:
      - !Ref PrivateRouteTable
    PolicyDocument:
      Version: '2012-10-17'
      Statement:
        - Effect: Allow
          Principal: '*'
          Action:
            - s3:GetObject
            - s3:ListBucket
          Resource:
            - arn:aws:s3:::your-bucket-name/*
            - arn:aws:s3:::your-bucket-name
```

### æš—å·åŒ–è¨­å®š

```python
# æš—å·åŒ–ã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—
def get_encrypted_object(bucket_name: str, object_key: str, kms_key_id: str):
    """
    KMSæš—å·åŒ–ã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å–å¾—
    """
    try:
        response = s3_client.get_object(
            Bucket=bucket_name,
            Key=object_key,
            # ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰æš—å·åŒ–æƒ…å ±
            ServerSideEncryption='aws:kms',
            SSEKMSKeyId=kms_key_id
        )
        
        return {
            'content': response['Body'].read(),
            'encryption_info': {
                'sse_algorithm': response.get('ServerSideEncryption', ''),
                'kms_key_id': response.get('SSEKMSKeyId', ''),
                'bucket_key_enabled': response.get('BucketKeyEnabled', False)
            }
        }
        
    except ClientError as e:
        if 'KMSAccessDenied' in str(e):
            logger.error("KMSæš—å·åŒ–ã‚­ãƒ¼ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“")
        raise
```

---

## ğŸš¨ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### åŒ…æ‹¬çš„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Ÿè£…

```python
import boto3
from botocore.exceptions import (
    ClientError, 
    NoCredentialsError, 
    PartialCredentialsError,
    ConnectTimeoutError,
    ReadTimeoutError
)
import time
import random
from functools import wraps

def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60):
    """
    æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã«ã‚ˆã‚‹ãƒªãƒˆãƒ©ã‚¤ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except ClientError as e:
                    error_code = e.response['Error']['Code']
                    
                    # ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                    if error_code in ['ServiceUnavailable', 'Throttling', 'SlowDown', 'RequestTimeout']:
                        if attempt < max_retries:
                            delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)
                            logger.warning(f"ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/{max_retries}: {delay:.1f}ç§’å¾Œã«å†è©¦è¡Œ")
                            time.sleep(delay)
                            continue
                    
                    # ãƒªãƒˆãƒ©ã‚¤ä¸å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                    raise
                except (ConnectTimeoutError, ReadTimeoutError) as e:
                    if attempt < max_retries:
                        delay = min(base_delay * (2 ** attempt), max_delay)
                        logger.warning(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/{max_retries}: {delay:.1f}ç§’å¾Œ")
                        time.sleep(delay)
                        continue
                    raise
                    
            return func(*args, **kwargs)  # æœ€å¾Œã®è©¦è¡Œ
        return wrapper
    return decorator

class ErrorHandlerS3Client:
    def __init__(self, region_name='ap-northeast-1'):
        self.s3_client = boto3.client('s3', region_name=region_name)
        self.logger = logging.getLogger(__name__)

    @retry_with_backoff(max_retries=3)
    def safe_get_object(self, bucket_name: str, object_key: str) -> Dict[str, Any]:
        """
        å®‰å…¨ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—ï¼ˆåŒ…æ‹¬çš„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        """
        try:
            response = self.s3_client.get_object(
                Bucket=bucket_name,
                Key=object_key
            )
            
            return {
                'success': True,
                'content': response['Body'].read(),
                'metadata': {
                    'content_type': response.get('ContentType', ''),
                    'content_length': response.get('ContentLength', 0),
                    'last_modified': response.get('LastModified', ''),
                    'etag': response.get('ETag', '')
                },
                'error': None
            }
            
        except ClientError as e:
            error_code = e.response['Error']['Code']
            error_message = e.response['Error']['Message']
            
            error_mapping = {
                'NoSuchKey': f"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“: {object_key}",
                'NoSuchBucket': f"ãƒã‚±ãƒƒãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“: {bucket_name}",
                'AccessDenied': f"ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: s3://{bucket_name}/{object_key}",
                'InvalidBucketName': f"ç„¡åŠ¹ãªãƒã‚±ãƒƒãƒˆå: {bucket_name}",
                'InvalidObjectName': f"ç„¡åŠ¹ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå: {object_key}",
                'RequestTimeout': "ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ",
                'ServiceUnavailable': "S3ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨ä¸å¯",
                'Throttling': "ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸ",
                'SlowDown': "ãƒªã‚¯ã‚¨ã‚¹ãƒˆé »åº¦ã‚’ä¸‹ã’ã¦ãã ã•ã„",
                'InternalError': "AWSå†…éƒ¨ã‚¨ãƒ©ãƒ¼"
            }
            
            user_message = error_mapping.get(error_code, f"S3ã‚¨ãƒ©ãƒ¼: {error_code}")
            self.logger.error(f"{user_message} - è©³ç´°: {error_message}")
            
            return {
                'success': False,
                'content': None,
                'metadata': {},
                'error': {
                    'code': error_code,
                    'message': user_message,
                    'aws_message': error_message,
                    'retriable': error_code in ['ServiceUnavailable', 'Throttling', 'SlowDown', 'RequestTimeout']
                }
            }
            
        except NoCredentialsError:
            error_msg = "AWSèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
            self.logger.error(error_msg)
            return {
                'success': False,
                'content': None,
                'metadata': {},
                'error': {
                    'code': 'NoCredentials',
                    'message': error_msg,
                    'aws_message': '',
                    'retriable': False
                }
            }
            
        except PartialCredentialsError:
            error_msg = "AWSèªè¨¼æƒ…å ±ãŒä¸å®Œå…¨ã§ã™"
            self.logger.error(error_msg)
            return {
                'success': False,
                'content': None,
                'metadata': {},
                'error': {
                    'code': 'PartialCredentials',
                    'message': error_msg,
                    'aws_message': '',
                    'retriable': False
                }
            }
            
        except Exception as e:
            error_msg = f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}"
            self.logger.error(error_msg)
            return {
                'success': False,
                'content': None,
                'metadata': {},
                'error': {
                    'code': 'UnexpectedError',
                    'message': error_msg,
                    'aws_message': str(e),
                    'retriable': True
                }
            }

# ä½¿ç”¨ä¾‹
def robust_file_processing():
    """
    å …ç‰¢ãªãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¾‹
    """
    error_handler = ErrorHandlerS3Client()
    
    result = error_handler.safe_get_object('your-bucket', 'your-object-key')
    
    if result['success']:
        # æˆåŠŸæ™‚ã®å‡¦ç†
        content = result['content']
        metadata = result['metadata']
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—æˆåŠŸ: {metadata['content_length']} bytes")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯
        process_file_content(content)
        
    else:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®å‡¦ç†
        error = result['error']
        print(f"ã‚¨ãƒ©ãƒ¼: {error['message']}")
        
        if error['retriable']:
            print("ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ã§ã™")
            # ã‚¢ãƒ©ãƒ¼ãƒˆã‚„ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡
            send_retry_alert(error)
        else:
            print("ãƒªãƒˆãƒ©ã‚¤ä¸å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ã§ã™")
            # ã‚¨ãƒ©ãƒ¼å ±å‘Šã‚„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            handle_permanent_error(error)
```

---

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### æ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š

```python
import boto3
from botocore.config import Config
from concurrent.futures import ThreadPoolExecutor
import threading

class OptimizedS3Client:
    def __init__(self, region_name='ap-northeast-1'):
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–è¨­å®š
        config = Config(
            retries={'max_attempts': 3, 'mode': 'adaptive'},
            max_pool_connections=50,  # æ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€å¤§æ•°
            region_name=region_name
        )
        
        self.s3_client = boto3.client('s3', config=config)
        self.thread_local = threading.local()
        
    def get_thread_local_client(self):
        """
        ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ã‚«ãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—
        """
        if not hasattr(self.thread_local, 's3_client'):
            config = Config(
                retries={'max_attempts': 3, 'mode': 'adaptive'},
                max_pool_connections=10
            )
            self.thread_local.s3_client = boto3.client('s3', config=config)
        return self.thread_local.s3_client

    def parallel_object_processing(self, bucket_name: str, object_keys: List[str], max_workers: int = 10):
        """
        ä¸¦åˆ—ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‡¦ç†
        """
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self._process_single_object, bucket_name, key): key 
                for key in object_keys
            }
            
            for future in as_completed(futures):
                object_key = futures[future]
                try:
                    result = future.result()
                    results.append({
                        'object_key': object_key,
                        'success': True,
                        'data': result
                    })
                except Exception as e:
                    results.append({
                        'object_key': object_key,
                        'success': False,
                        'error': str(e)
                    })
        
        return results
    
    def _process_single_object(self, bucket_name: str, object_key: str):
        """
        å˜ä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‡¦ç†
        """
        client = self.get_thread_local_client()
        
        response = client.get_object(
            Bucket=bucket_name,
            Key=object_key
        )
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯
        content = response['Body'].read()
        
        return {
            'size': len(content),
            'content_type': response.get('ContentType', ''),
            'last_modified': response.get('LastModified', '')
        }
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°å®Ÿè£…

```python
import redis
import json
import hashlib
from typing import Optional, Dict, Any
from datetime import datetime, timedelta

class CachedS3Client:
    def __init__(self, region_name='ap-northeast-1', redis_host='localhost', redis_port=6379):
        self.s3_client = boto3.client('s3', region_name=region_name)
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
        self.default_ttl = 3600  # 1æ™‚é–“
        
    def _generate_cache_key(self, bucket_name: str, object_key: str) -> str:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
        """
        key_string = f"s3:{bucket_name}:{object_key}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def get_object_cached(self, bucket_name: str, object_key: str, ttl: Optional[int] = None) -> Dict[str, Any]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—
        """
        cache_key = self._generate_cache_key(bucket_name, object_key)
        ttl = ttl or self.default_ttl
        
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
            cached_data = self.redis_client.get(cache_key)
            if cached_data:
                logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {object_key}")
                return json.loads(cached_data)
            
            logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã€S3ã‹ã‚‰å–å¾—: {object_key}")
            
            # S3ã‹ã‚‰å–å¾—
            response = self.s3_client.get_object(
                Bucket=bucket_name,
                Key=object_key
            )
            
            content = response['Body'].read()
            
            result = {
                'content': content.decode('utf-8', errors='ignore'),  # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æƒ³å®š
                'metadata': {
                    'content_type': response.get('ContentType', ''),
                    'content_length': response.get('ContentLength', 0),
                    'last_modified': response.get('LastModified', '').isoformat(),
                    'etag': response.get('ETag', '')
                },
                'cached_at': datetime.utcnow().isoformat()
            }
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.redis_client.setex(
                cache_key, 
                ttl, 
                json.dumps(result, default=str)
            )
            
            return result
            
        except redis.RedisError as e:
            logger.warning(f"Redis ã‚¨ãƒ©ãƒ¼ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã§å–å¾—: {str(e)}")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç›´æ¥S3ã‹ã‚‰å–å¾—
            return self._get_object_direct(bucket_name, object_key)
        except Exception as e:
            logger.error(f"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def invalidate_cache(self, bucket_name: str, object_key: str) -> bool:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–
        """
        cache_key = self._generate_cache_key(bucket_name, object_key)
        try:
            return bool(self.redis_client.delete(cache_key))
        except redis.RedisError as e:
            logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
```

---

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
import boto3
import requests
from botocore.exceptions import ClientError, NoCredentialsError
import subprocess
import sys

class S3AccessDiagnostics:
    def __init__(self):
        self.results = []
        
    def run_all_diagnostics(self):
        """
        å…¨è¨ºæ–­å®Ÿè¡Œ
        """
        print("=== EC2ã‹ã‚‰S3ã‚¢ã‚¯ã‚»ã‚¹è¨ºæ–­é–‹å§‹ ===\n")
        
        # 1. EC2ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        self.check_ec2_metadata()
        
        # 2. IAMãƒ­ãƒ¼ãƒ«ç¢ºèª
        self.check_iam_role()
        
        # 3. AWSèªè¨¼æƒ…å ±ç¢ºèª
        self.check_aws_credentials()
        
        # 4. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šç¢ºèª
        self.check_network_connectivity()
        
        # 5. S3ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ
        self.test_s3_access()
        
        # 6. æ¨©é™ãƒ†ã‚¹ãƒˆ
        self.test_permissions()
        
        print("\n=== è¨ºæ–­çµæœã‚µãƒãƒªãƒ¼ ===")
        self.print_summary()
    
    def check_ec2_metadata(self):
        """
        EC2ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª
        """
        print("1. EC2ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª")
        try:
            # IMDSv2ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—
            token_response = requests.put(
                'http://169.254.169.254/latest/api/token',
                headers={'X-aws-ec2-metadata-token-ttl-seconds': '21600'},
                timeout=2
            )
            
            if token_response.status_code == 200:
                token = token_response.text
                
                # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æƒ…å ±å–å¾—
                metadata_response = requests.get(
                    'http://169.254.169.254/latest/meta-data/instance-id',
                    headers={'X-aws-ec2-metadata-token': token},
                    timeout=2
                )
                
                if metadata_response.status_code == 200:
                    instance_id = metadata_response.text
                    print(f"   âœ… EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: {instance_id}")
                    self.results.append(("EC2ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿", True, f"ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: {instance_id}"))
                else:
                    print(f"   âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {metadata_response.status_code}")
                    self.results.append(("EC2ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿", False, "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—"))
            else:
                print(f"   âŒ IMDSãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å¤±æ•—: {token_response.status_code}")
                self.results.append(("EC2ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿", False, "IMDSãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å¤±æ•—"))
                
        except requests.exceptions.RequestException as e:
            print(f"   âŒ EC2ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒ¼ãƒ“ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“: {str(e)}")
            self.results.append(("EC2ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿", False, f"ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯: {str(e)}"))
        
        print()
    
    def check_iam_role(self):
        """
        IAMãƒ­ãƒ¼ãƒ«ç¢ºèª
        """
        print("2. IAMãƒ­ãƒ¼ãƒ«ç¢ºèª")
        try:
            token_response = requests.put(
                'http://169.254.169.254/latest/api/token',
                headers={'X-aws-ec2-metadata-token-ttl-seconds': '21600'},
                timeout=2
            )
            
            if token_response.status_code == 200:
                token = token_response.text
                
                # IAMãƒ­ãƒ¼ãƒ«æƒ…å ±å–å¾—
                iam_response = requests.get(
                    'http://169.254.169.254/latest/meta-data/iam/security-credentials/',
                    headers={'X-aws-ec2-metadata-token': token},
                    timeout=2
                )
                
                if iam_response.status_code == 200:
                    role_name = iam_response.text
                    print(f"   âœ… IAMãƒ­ãƒ¼ãƒ«: {role_name}")
                    
                    # èªè¨¼æƒ…å ±å–å¾—
                    creds_response = requests.get(
                        f'http://169.254.169.254/latest/meta-data/iam/security-credentials/{role_name}',
                        headers={'X-aws-ec2-metadata-token': token},
                        timeout=2
                    )
                    
                    if creds_response.status_code == 200:
                        creds = creds_response.json()
                        print(f"   âœ… AccessKeyId: {creds['AccessKeyId'][:8]}...")
                        print(f"   âœ… æœ‰åŠ¹æœŸé™: {creds['Expiration']}")
                        self.results.append(("IAMãƒ­ãƒ¼ãƒ«", True, f"ãƒ­ãƒ¼ãƒ«: {role_name}"))
                    else:
                        print(f"   âŒ èªè¨¼æƒ…å ±å–å¾—å¤±æ•—: {creds_response.status_code}")
                        self.results.append(("IAMãƒ­ãƒ¼ãƒ«", False, "èªè¨¼æƒ…å ±å–å¾—å¤±æ•—"))
                else:
                    print(f"   âŒ IAMãƒ­ãƒ¼ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                    self.results.append(("IAMãƒ­ãƒ¼ãƒ«", False, "IAMãƒ­ãƒ¼ãƒ«æœªè¨­å®š"))
        except Exception as e:
            print(f"   âŒ IAMãƒ­ãƒ¼ãƒ«ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.results.append(("IAMãƒ­ãƒ¼ãƒ«", False, f"ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}"))
        
        print()
    
    def check_aws_credentials(self):
        """
        AWSèªè¨¼æƒ…å ±ç¢ºèª
        """
        print("3. AWSèªè¨¼æƒ…å ±ç¢ºèª")
        try:
            sts = boto3.client('sts')
            identity = sts.get_caller_identity()
            
            print(f"   âœ… Account: {identity['Account']}")
            print(f"   âœ… User/Role ARN: {identity['Arn']}")
            print(f"   âœ… User ID: {identity['UserId']}")
            
            self.results.append(("AWSèªè¨¼", True, f"Account: {identity['Account']}"))
            
        except NoCredentialsError:
            print("   âŒ AWSèªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            self.results.append(("AWSèªè¨¼", False, "èªè¨¼æƒ…å ±ãªã—"))
        except Exception as e:
            print(f"   âŒ èªè¨¼æƒ…å ±ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.results.append(("AWSèªè¨¼", False, f"ã‚¨ãƒ©ãƒ¼: {str(e)}"))
        
        print()
    
    def check_network_connectivity(self):
        """
        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šç¢ºèª
        """
        print("4. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šç¢ºèª")
        
        # S3ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¥ç¶šãƒ†ã‚¹ãƒˆ
        endpoints = [
            "s3.amazonaws.com",
            "s3.ap-northeast-1.amazonaws.com"
        ]
        
        for endpoint in endpoints:
            try:
                response = requests.get(f"https://{endpoint}", timeout=5)
                print(f"   âœ… {endpoint}: æ¥ç¶šOK ({response.status_code})")
                self.results.append((f"æ¥ç¶š-{endpoint}", True, "æ¥ç¶šOK"))
            except Exception as e:
                print(f"   âŒ {endpoint}: æ¥ç¶šNG ({str(e)})")
                self.results.append((f"æ¥ç¶š-{endpoint}", False, f"æ¥ç¶šNG: {str(e)}"))
        
        print()
    
    def test_s3_access(self):
        """
        åŸºæœ¬çš„ãªS3ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ
        """
        print("5. S3ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ")
        try:
            s3 = boto3.client('s3')
            
            # ãƒã‚±ãƒƒãƒˆä¸€è¦§å–å¾—ãƒ†ã‚¹ãƒˆ
            response = s3.list_buckets()
            bucket_count = len(response['Buckets'])
            print(f"   âœ… ãƒã‚±ãƒƒãƒˆä¸€è¦§å–å¾—æˆåŠŸ: {bucket_count}å€‹")
            self.results.append(("S3ã‚¢ã‚¯ã‚»ã‚¹", True, f"ãƒã‚±ãƒƒãƒˆ: {bucket_count}å€‹"))
            
        except ClientError as e:
            error_code = e.response['Error']['Code']
            print(f"   âŒ S3ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {error_code}")
            self.results.append(("S3ã‚¢ã‚¯ã‚»ã‚¹", False, f"ã‚¨ãƒ©ãƒ¼: {error_code}"))
        except Exception as e:
            print(f"   âŒ S3ã‚¢ã‚¯ã‚»ã‚¹äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.results.append(("S3ã‚¢ã‚¯ã‚»ã‚¹", False, f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}"))
        
        print()
    
    def test_permissions(self, test_bucket=None):
        """
        æ¨©é™ãƒ†ã‚¹ãƒˆ
        """
        print("6. S3æ¨©é™ãƒ†ã‚¹ãƒˆ")
        
        if not test_bucket:
            print("   â„¹ï¸  ãƒ†ã‚¹ãƒˆãƒã‚±ãƒƒãƒˆåãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ¨©é™ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return
        
        s3 = boto3.client('s3')
        
        # GetObjectæ¨©é™ãƒ†ã‚¹ãƒˆ
        try:
            s3.head_object(Bucket=test_bucket, Key='test-key')
            print(f"   âœ… {test_bucket}: HeadObject æ¨©é™OK")
            self.results.append((f"æ¨©é™-{test_bucket}", True, "HeadObject OK"))
        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code == 'NotFound':
                print(f"   âœ… {test_bucket}: GetObject æ¨©é™OK (ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæœªå­˜åœ¨)")
                self.results.append((f"æ¨©é™-{test_bucket}", True, "GetObject OK"))
            elif error_code == 'Forbidden':
                print(f"   âŒ {test_bucket}: GetObject æ¨©é™NG")
                self.results.append((f"æ¨©é™-{test_bucket}", False, "GetObject æ¨©é™ãªã—"))
            else:
                print(f"   âŒ {test_bucket}: æ¨©é™ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {error_code}")
                self.results.append((f"æ¨©é™-{test_bucket}", False, f"ã‚¨ãƒ©ãƒ¼: {error_code}"))
        
        print()
    
    def print_summary(self):
        """
        çµæœã‚µãƒãƒªãƒ¼å‡ºåŠ›
        """
        success_count = sum(1 for _, success, _ in self.results if success)
        total_count = len(self.results)
        
        print(f"æˆåŠŸ: {success_count}/{total_count}")
        print()
        
        print("è©³ç´°:")
        for test_name, success, details in self.results:
            status = "âœ…" if success else "âŒ"
            print(f"  {status} {test_name}: {details}")
        
        print()
        
        if success_count == total_count:
            print("ğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
        else:
            print("âš ï¸  ã„ãã¤ã‹ã®é …ç›®ã§å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚")
            print("   ä¸Šè¨˜ã®âŒé …ç›®ã‚’ç¢ºèªã—ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚")

# ä½¿ç”¨æ–¹æ³•
if __name__ == "__main__":
    diagnostics = S3AccessDiagnostics()
    
    # ãƒ†ã‚¹ãƒˆãƒã‚±ãƒƒãƒˆæŒ‡å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    test_bucket = input("æ¨©é™ãƒ†ã‚¹ãƒˆç”¨ãƒã‚±ãƒƒãƒˆåï¼ˆEnter ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰: ").strip() or None
    
    diagnostics.run_all_diagnostics()
    if test_bucket:
        diagnostics.test_permissions(test_bucket)
```

### ä¸€èˆ¬çš„ãªå•é¡Œã¨è§£æ±ºæ–¹æ³•

| å•é¡Œ | ç—‡çŠ¶ | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| **èªè¨¼ã‚¨ãƒ©ãƒ¼** | NoCredentialsError | IAMãƒ­ãƒ¼ãƒ«ã®è¨­å®šç¢ºèªã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª |
| **ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦** | AccessDenied | IAMãƒãƒªã‚·ãƒ¼ã®æ¨©é™ç¢ºèªã€ãƒã‚±ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã®ç¢ºèª |
| **ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæœªå­˜åœ¨** | NoSuchKey | ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã®ç¢ºèªã€ãƒã‚±ãƒƒãƒˆåã®ç¢ºèª |
| **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼** | ConnectTimeoutError | ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—ã€NACLã®ç¢ºèªã€VPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š |
| **ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°** | Throttling | ãƒªã‚¯ã‚¨ã‚¹ãƒˆé »åº¦ã®èª¿æ•´ã€æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã®å®Ÿè£… |

---

## ğŸ¯ ã¾ã¨ã‚

æœ¬ã‚¬ã‚¤ãƒ‰ã§ã¯ã€EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å®‰å…¨ã‹ã¤åŠ¹ç‡çš„ã«å–å¾—ã™ã‚‹æ–¹æ³•ã‚’åŒ…æ‹¬çš„ã«èª¬æ˜ã—ã¾ã—ãŸã€‚

### ğŸ”‘ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

1. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ**: IAMãƒ­ãƒ¼ãƒ«ã«ã‚ˆã‚‹ä¸€æ™‚çš„èªè¨¼æƒ…å ±ã®ä½¿ç”¨
2. **æœ€å°æ¨©é™ã®åŸå‰‡**: å¿…è¦æœ€å°é™ã®æ¨©é™ã®ã¿ä»˜ä¸
3. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼å‡¦ç†ã¨ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
4. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**: æ¥ç¶šãƒ—ãƒ¼ãƒ«ã€ä¸¦åˆ—å‡¦ç†ã€ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
5. **ç›£è¦–ã¨ãƒ­ã‚®ãƒ³ã‚°**: é©åˆ‡ãªãƒ­ã‚°å‡ºåŠ›ã¨ç›£è¦–è¨­å®š

### ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

- CloudWatchç›£è¦–ã®è¨­å®š
- X-Rayåˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®å°å…¥  
- AWS Config ã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ç›£è¦–
- AWS GuardDuty ã«ã‚ˆã‚‹è„…å¨æ¤œå‡ºã®æœ‰åŠ¹åŒ–

---

*ã“ã®ã‚¬ã‚¤ãƒ‰ãŒã€å®‰å…¨ã§åŠ¹ç‡çš„ãªS3ã‚¢ã‚¯ã‚»ã‚¹å®Ÿè£…ã®å‚è€ƒã«ãªã‚Œã°å¹¸ã„ã§ã™ï¼* ğŸ‰